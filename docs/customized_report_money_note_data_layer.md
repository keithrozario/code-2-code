# Project Modernization Report

# Table of Contents

1. [Report Metadata](#report-metadata)
1. [Summary](#summary)
1. [About this Project](#about-this-project)
    1. [Assessment Overview](#assessment-overview)
    1. [Data Layer Architecture](#data-layer-architecture)
    1. [Data Access and Management](#data-access-and-management)
    1. [Security Considerations](#security-considerations)
1. [Assessment Findings](#assessment-findings)
    1. [Modernization Opportunities](#modernization-opportunities)
    1. [Modernization Challenges](#modernization-challenges)
    1. [Modernization Strategies](#modernization-strategies)


## Report Metadata

**Project:** moneynote-api

**Report ID:** CM-20250911-141011

**Generated:** Sep 11, 2025, 2:10 PM

## Summary
### Executive Summary

The MoneyNote API is a monolithic personal finance application developed with Java and the Spring Boot framework. It provides core bookkeeping functionalities, allowing users to manage accounts, track transactions, and generate financial reports. The entire system is built upon a single MySQL database, which serves as the data store for all business entities and for storing user-uploaded file attachments directly as binary large objects (BLOBs).

### Key Findings

The assessment has identified several critical issues that pose significant risks to the application's security, scalability, and maintainability.

*   **Critical Security Vulnerabilities**: The application's most severe flaw is a **hardcoded secret key** used for signing JSON Web Tokens (JWTs), which allows anyone with source code access to forge authentication tokens and gain unauthorized access to any user's data. Furthermore, data is not encrypted in transit between the application and the database, and a default password exists in the configuration file.
*   **Architectural and Scalability Bottlenecks**: The monolithic architecture, coupled with a single MySQL database instance, creates a single point of failure and a major performance bottleneck. Storing file attachments directly in the database further degrades performance, increases costs, and complicates backup procedures.
*   **High Operational Risk**: The application lacks robust operational practices. Database schema migrations are handled by a brittle, manual SQL script, and the documented backup process is an error-prone manual procedure. This creates a high risk of data loss, corruption, and extended downtime.
*   **Lack of a Caching Strategy**: The absence of a distributed caching layer means frequently accessed data is repeatedly fetched from the database, increasing load and latency. The current in-memory cache and stateful session management hinder horizontal scalability.

### Strategic Recommendations

To address these findings and prepare the application for a successful future on Google Cloud, a phased modernization approach is recommended.

*   **Phase 1: Foundational Migration and Security Remediation**:
    *   **Mitigate Critical Risks**: Immediately externalize the hardcoded JWT secret and all other credentials using **Google Cloud Secret Manager**.
    *   **Lift and Shift the Database**: Migrate the existing MySQL database to a fully managed **Google Cloud SQL for MySQL** instance. This will immediately improve reliability, automate backups, and reduce operational overhead.
    *   **Modernize Storage**: Refactor the application to store file attachments in **Google Cloud Storage** instead of the database. This will significantly improve performance, reduce database size, and lower storage costs.
    *   **Adopt Robust Schema Management**: Replace the manual SQL script runner with a dedicated database migration tool like **Flyway** or **Liquibase** to ensure safe, version-controlled schema updates.

*   **Phase 2: Cloud-Native Optimization**:
    *   **Enhance Performance and Scalability**: Deploy the containerized application on **Google Cloud Run** to leverage serverless computing for improved scalability and cost-efficiency. Implement a distributed caching layer with **Google Cloud Memorystore for Redis** to reduce database load and support a stateless architecture.
    *   **Separate Analytical Workloads**: To prevent reporting queries from impacting application performance, establish a data pipeline to replicate transactional data to **Google BigQuery**. All reporting and analytical features should be refactored to query BigQuery, isolating OLTP and OLAP workloads.

### Report Information

#### Estimated Application Metrics (AI-generated)

| Metric | Value |
| :--- | :--- |
| Scanned Lines of Code (LOC) | 13,559 |
| Scanned Files | 242 |

### Generation Details

 - **Tool**: Google Cloud Code Modernization Assessment Tool v0.9.0
 - **Modelset**: gemini-2.5-pro
 - **Generated By**: keith_krozario_altostrat_com

### Command Line Options

The report was generated using the following CLI command:

 ```plaintext
codmod create data-layer \
    -c moneynote-api/ \
    --format markdown \
    -o customized_report_money_note_data_layer.md
 ```

### Notice Regarding AI-Generated Assessment

 - This report leverages advanced AI for analysis, providing rapid, evidence-based insights. Please note that results are AI-derived estimates and may vary between assessments, even for the same codebase. Human review and validation are crucial for all findings and recommendations.

## About this Project
### Assessment Overview
The data layer of the MoneyNote API is built on a traditional relational database model, with application-level logic for caching, data processing, and handling binary data. The architecture is self-contained, storing all application data, including file attachments, within a single MySQL database.

#### Databases

The application exclusively uses a **MySQL** relational database as its persistence layer. All core business data, including user information, financial accounts, transactions, and metadata, is stored in this database.

-   **Technology**: The interaction with the database is managed through **Spring Data JPA**, which provides an abstraction over standard JDBC. For complex queries, the application utilizes **QueryDSL** to build type-safe SQL-like queries within the Java code [^1].
-   **Configuration**: The database connection is configured in the `application.properties` file. It specifies the JDBC URL, credentials, and driver class for MySQL. The configuration uses environment variables for sensitive details like host, user, and password, allowing for flexibility in different deployment environments [^2].
-   **Schema Management**: The application uses Hibernate's `ddl-auto=update` property, which automatically updates the database schema based on entity definitions. Additionally, a `SqlScriptRunner` component executes a raw SQL script (`1.sql`) on startup to perform data migrations that are not handled by Hibernate, such as updating enum values or altering column types [^3].
-   **Storage Engine**: Ancillary files suggest the use of the `MyISAM` storage engine, which is notable for its different transactional and locking characteristics compared to the more common `InnoDB` engine [^4, ^5].

#### Storage

File storage for transaction attachments (e.g., receipts, images) is handled directly within the primary MySQL database.

-   **Implementation**: The `FlowFile` entity is designed to store binary data. It contains a `byte[] data` field annotated with `@Lob` and mapped to a `LONGBLOB` column type in the database [^6].
-   **Strategy**: This approach keeps the application self-contained, as it avoids reliance on an external file system or cloud storage service. However, storing large binary files in a relational database can impact performance, backup times, and overall database size. The file upload size is capped at 100MB via application properties [^7].

#### Caching

The application employs a simple, in-memory, application-level caching mechanism rather than a dedicated distributed cache service like Redis or Memcached.

-   **Mechanism**: An `ApplicationScopeBean` is used to cache data that is frequently accessed and rarely changes. This bean has an application-wide scope, meaning its instance and the data it holds are shared across all user sessions [^8].
-   **Cached Data**:
    *   **Currency Rates**: Loaded on startup by `CurrencyDataLoader` from a `currency.json` file and periodically refreshed from an external API by `CurrencyRemoteDataLoader` [^9, ^10].
    *   **Book Templates**: Loaded on startup by `BookTplDataLoader` from a `book_tpl.json` file to provide default structures for new bookkeeping ledgers [^11].

This strategy is effective for small-scale, read-heavy data but does not provide the scalability or eviction policies of a dedicated caching solution.

#### Data Processing

Data processing is performed in real-time within the application's business logic, primarily for reporting and data aggregation. There are no separate batch processing jobs or ETL pipelines.

-   **Reporting**: The `ReportService` is responsible for generating various financial reports. It directly queries the database using JPA and QueryDSL, performing aggregations on transaction data (`BalanceFlow`) to calculate metrics by category, tag, and payee [^12]. All calculations are performed on-the-fly when a report is requested.
-   **Data Aggregation**: The service computes sums and percentages for financial data, such as total expenses and income, and calculates asset and debt distributions for balance reports. This logic resides entirely within the service layer.

#### Key Data Entities

The application's data model revolves around several core entities that represent users, their financial structure, and their transactions.

| Entity | Purpose | Key Relationships |
| :--- | :--- | :--- |
| **User** | Represents an application user, holding their credentials and default preferences. | - Has a default `Group` and `Book`.<br>- Linked to multiple `Group`s via `UserGroupRelation`. |
| **Group** | A container for collaborative bookkeeping, enabling multiple users to share financial data. | - Created by a `User`.<br>- Contains `Book`s and `Account`s.<br>- Manages user roles and permissions via `UserGroupRelation`. |
| **Book** | A ledger for recording transactions. Each book has its own set of categories, tags, and payees. | - Belongs to a `Group`.<br>- Contains `Category`, `Tag`, and `Payee` entities.<br>- Is the primary scope for `BalanceFlow` entries. |
| **Account** | Represents a real-world financial account (e.g., bank account, credit card, asset). | - Belongs to a `Group`.<br>- Holds a `balance` and has attributes defining its capabilities (e.g., `canExpense`, `canIncome`).<br>- Is the source or destination for `BalanceFlow` transactions. |
| **BalanceFlow** | The central transaction entity, recording all financial movements like expenses, incomes, transfers, and adjustments. | - Belongs to a `Book` and `Group`.<br>- Links to `Account`(s), `Category`(ies), `Tag`(s), `Payee`, and `FlowFile`(s). |
| **Category, Tag** | Hierarchical entities used to classify and organize transactions. Both extend `TreeEntity` to support parent-child relationships. | - Belong to a `Book`.<br>- Linked to `BalanceFlow` via `CategoryRelation` and `TagRelation`. |
| **Payee** | Represents the other party in a transaction (e.g., a store, a person). | - Belongs to a `Book`.<br>- Can be associated with an `EXPENSE` or `INCOME` type `BalanceFlow`. |

[^1]: build.gradle: `querydsl-jpa` - Dependency for using QueryDSL for type-safe database queries.
[^2]: src/main/resources/application.properties: `spring.datasource.url` - Defines the JDBC connection string for the MySQL database.
[^3]: src/main/java/cn/biq/mn/SqlScriptRunner.java: `run` - Executes the `1.sql` script at application startup to handle manual database migrations.
[^4]: src/main/resources/hibernate.properties: `hibernate.dialect.storage_engine=myisam` - Specifies MyISAM as the desired storage engine for Hibernate.
[^5]: notes/data_backup.txt: `要把存储引擎改为 MyISAM。` - A note indicating a preference or requirement for the MyISAM storage engine.
[^6]: src/main/java/cn/biq/mn/flowfile/FlowFile.java: `data` - A `byte[]` field annotated with `@Lob` to store file data directly in the database.
[^7]: src/main/resources/application.properties: `spring.servlet.multipart.max-file-size=100MB` - Configuration property setting the maximum size for uploaded files.
[^8]: src/main/java/cn/biq/mn/bean/ApplicationScopeBean.java: `@ApplicationScope` - A Spring bean that holds application-wide data, acting as an in-memory cache.
[^9]: src/main/java/cn/biq/mn/currency/CurrencyDataLoader.java: `run` - Loads currency information from a JSON file into the `ApplicationScopeBean` on startup.
[^10]: src/main/java/cn/biq/mn/currency/CurrencyRemoteDataLoader.java: `run` - Refreshes currency exchange rates from a remote API after startup.
[^11]: src/main/java/cn/biq/mn/book/tpl/BookTplDataLoader.java: `run` - Loads book templates from a JSON file into the `ApplicationScopeBean` on startup.
[^12]: src/main/java/cn/biq/mn/report/ReportService.java: `reportCategory` - A method that aggregates financial data from `BalanceFlow` entities to generate reports.

### Data Layer Architecture
#### Data Layer Architecture

The data layer of the MoneyNote API is built on a traditional, monolithic architecture using a relational database. It leverages modern Java frameworks to manage data persistence, providing a solid foundation for the application's core functionalities. However, its monolithic nature presents certain challenges for cloud-native scalability and modernization.

##### Components and Roles

The data layer is composed of several key components that work together to manage the application's state.

| Component | Technology/Framework | Role |
| :--- | :--- | :--- |
| **Database** | MySQL | The primary data store for the application. It houses all user data, including accounts, transactions, and configurations. The database connection is configured to be flexible, using environment variables for host, port, and credentials, which is suitable for containerized deployments [^1]. |
| **Connection Pooling** | HikariCP | Manages a pool of database connections to improve performance and resource management by reusing connections instead of creating new ones for each request [^2]. |
| **ORM Framework** | Spring Data JPA / Hibernate | Provides a high-level abstraction over JDBC for object-relational mapping. It maps Java entity classes to database tables, simplifying data access and manipulation [^3]. The `spring.jpa.hibernate.ddl-auto=update` configuration indicates that Hibernate manages schema updates automatically, which is convenient for development but not recommended for production environments. |
| **Query Language**| QueryDSL | Used for building type-safe SQL-like queries. This allows for the dynamic construction of complex queries in a fluent and maintainable way, as seen in various `QueryForm` classes [^4]. |
| **Data Access Objects (DAO)** | Spring Data Repositories | A set of interfaces (e.g., `AccountRepository`, `BookRepository`) that extend a custom `BaseRepository`. They abstract the data access logic, providing standard CRUD operations and the ability to execute complex queries defined with QueryDSL [^5]. |
| **Schema/Data Initialization** | Custom `ApplicationRunner` | A custom runner executes a raw SQL script (`1.sql`) upon application startup using `JdbcTemplate`. This script handles schema alterations and data migrations, such as updating enum values and altering column types [^6]. |

###### Cloud Services Identification
The codebase does not contain direct integrations with specific Google Cloud, AWS, or Azure managed database services. The JDBC configuration is generic, pointing to a MySQL instance defined by environment variables. The CI/CD pipelines build Docker images and push them to **Aliyun Container Registry (ACR)** [^7], suggesting the application is likely deployed on Alibaba Cloud. However, the database itself could be a self-managed instance on a VM or a managed service. The architecture's use of containerization and environment variables for configuration makes it portable and adaptable for migration to a managed database service like **Google Cloud SQL**.

##### Data Model

The application employs a relational data model to structure its bookkeeping data. Core entities are interconnected to represent users, groups, financial accounts, and transactions.

| Entity (Table Name) | Description | Key Relationships |
| :--- | :--- | :--- |
| `User` (`t_user_user`) | Represents an application user with credentials and personal information. | One-to-Many with `UserGroupRelation`. Has a default `Group` and `Book`. |
| `Group` (`t_user_group`) | A container for collaborative bookkeeping, allowing multiple users. | Many-to-One with `User` (creator). One-to-Many with `UserGroupRelation`. Has a default `Book`. |
| `UserGroupRelation` (`t_user_user_group_relation`) | A join table defining a `User`'s role within a `Group` (e.g., owner, operator). | Many-to-One with `User` and `Group`. |
| `Book` (`t_user_book`) | Represents a ledger or a set of books, belonging to a `Group`. | Many-to-One with `Group`. Holds default accounts and categories. One-to-Many with `Category`, `Tag`, and `Payee`. |
| `Account` (`t_user_account`) | Represents a financial account (e.g., checking, credit card), belonging to a `Group`. | Many-to-One with `Group`. Tracks `balance`, `creditLimit`, and other attributes. |
| `BalanceFlow` (`t_user_balance_flow`) | The central transaction entity, recording expenses, incomes, and transfers. | Many-to-One with `Book`, `Account` (from), `Account` (to), `Payee`, and `User` (creator). Links to categories and tags via relation tables. |
| `Category` (`t_user_category`) | A hierarchical structure (`TreeEntity`) for classifying transactions. Belongs to a `Book`. | Many-to-One with `Book`. Can have a parent `Category`. |
| `Tag` (`t_user_tag`) | A hierarchical structure (`TreeEntity`) for adding metadata to transactions. Belongs to a `Book`. | Many-to-One with `Book`. Can have a parent `Tag`. |
| `CategoryRelation` (`t_user_category_relation`) | Join table linking a `BalanceFlow` to a `Category` and storing the amount for that category. | Many-to-One with `Category` and `BalanceFlow`. |
| `TagRelation` (`t_user_tag_relation`) | Join table linking a `BalanceFlow` to a `Tag`. | Many-to-One with `Tag` and `BalanceFlow`. |
| `FlowFile` (`t_flow_file`) | Stores file attachments (e.g., receipts) for a `BalanceFlow` as a `LONGBLOB`. | Many-to-One with `BalanceFlow`. |

##### Data Flow

Data moves bidirectionally between the API layer and the MySQL database, facilitated by the service and repository layers.

| Source | Destination | Operation | Description |
| :--- | :--- | :--- | :--- |
| API: `POST /accounts` (`AccountController`) | Database: `t_user_account` table | **Write (Insert)** | A new account is created via the API. The `AccountService` maps the request form to an `Account` entity and persists it using `AccountRepository.save()` [^8]. |
| API: `GET /balance-flows` (`BalanceFlowController`) | API: `GET /balance-flows` | **Read** | Transaction data is queried from the `t_user_balance_flow` table via `BalanceFlowRepository.findAll()`. The service layer then maps the entities to `BalanceFlowDetails` DTOs for the API response, handling pagination [^9]. |
| API: `PATCH /balance-flows/{id}/confirm` (`BalanceFlowController`) | Database: `t_user_account`, `t_user_balance_flow` tables | **Read-Modify-Write** | Confirming a transaction reads the `BalanceFlow` and associated `Account` entities. It updates the account balance based on the flow type (e.g., expense, income) and saves the modified account. The `confirm` status of the `BalanceFlow` is also updated [^10]. |
| Code: `SqlScriptRunner` | Database: Multiple tables | **Write (Update/Alter)** | On application startup, the `SqlScriptRunner` executes the `1.sql` script to apply database schema changes and data migrations, ensuring the database aligns with the current application version [^6]. |
| Database: `t_user_category` and related tables | API: `GET /reports/expense-category` (`ReportController`) | **Read (Aggregate)** | The reporting endpoint triggers complex aggregations. The `ReportService` queries multiple tables, including `t_user_category_relation`, filters by various criteria, and calculates sums to generate category-based financial reports [^11]. |

##### Scalability and Performance

The current architecture includes several standard mechanisms to support performance, but its scalability is constrained by its monolithic design.

**Strengths:**
*   **Connection Pooling**: HikariCP is configured to efficiently manage database connections, reducing latency for individual requests [^2].
*   **Pagination**: The API consistently uses pagination (`Pageable`) for list-based endpoints, preventing the system from loading excessively large datasets into memory [^12].
*   **Lazy Loading**: The data model extensively uses `FetchType.LAZY` for entity relationships, which avoids loading unnecessary associated data and helps prevent performance degradation from over-fetching [^13].
*   **Type-Safe Queries**: QueryDSL enables the construction of dynamic queries, which are translated into efficient SQL, avoiding the overhead of in-application filtering of large collections.
*   **Batch Writes**: Hibernate's JDBC batch size is configured (`spring.jpa.properties.hibernate.jdbc.batch_size=50`), which can improve the performance of bulk insert and update operations [^14].

**Weaknesses:**
*   **Monolithic Bottleneck**: As a single application connected to a single database instance, both the API and the database can become performance bottlenecks under high load.
*   **N+1 Query Risk**: While lazy loading is beneficial, it introduces the risk of the "N+1 select" problem. If related entities are accessed within a loop, it can trigger numerous individual queries. The application does not appear to use mitigating strategies like Entity Graphs or explicit `JOIN FETCH` clauses in all necessary places, which could lead to performance issues in complex data retrieval scenarios.
*   **Single Database Instance**: The architecture relies on a single MySQL database. This presents a single point of failure and a scaling bottleneck. For a high-availability, scalable system, this would need to be replaced with a managed, replicable database solution like **Google Cloud SQL** with read replicas.

##### Key Challenges and Limitations

Modernizing this data layer for a cloud environment will require addressing several architectural limitations.

*   **Inadequate Schema Management**: The application relies on `ddl-auto=update` and a custom SQL script runner (`SqlScriptRunner`) [^6] for database migrations. This approach is risky for production environments, as it can lead to unintended schema modifications and potential data loss. Adopting a robust database migration tool like **Flyway** or **Liquibase** is critical for safe, version-controlled schema management in a CI/CD pipeline.
*   **Tight Coupling**: The data access logic is tightly integrated within the monolithic service layer. This makes it challenging to decompose the application into microservices, as many services would likely need to share the same database, leading to a distributed monolith anti-pattern. A data decomposition strategy would be a prerequisite for a true microservices architecture.
*   **Limited Caching Strategy**: The system lacks a comprehensive caching layer. Only currency data is cached at the application scope [^15]. Frequently accessed, slowly changing data (e.g., user profiles, book configurations) is fetched from the database on each request. Introducing a distributed cache like **Redis** (available as **Memorystore for Redis** on Google Cloud) would significantly reduce database load and improve response times.
*   **Transactional Complexity**: Business logic, such as confirming a transaction, involves multiple database writes within a single transactional method (`BalanceFlowService.confirmBalance`) [^10]. While this ensures data consistency in a monolith, it becomes a major challenge in a distributed microservices architecture, where distributed transaction patterns (e.g., Saga) would be required.

[^1]: application.properties: `spring.datasource.url` - Defines the JDBC connection string using environment variables for database credentials and location.
[^2]: application.properties: `spring.datasource.type=com.zaxxer.hikari.HikariDataSource` - Specifies HikariCP as the connection pool implementation.
[^3]: build.gradle: `implementation 'org.springframework.boot:spring-boot-starter-data-jpa'` - Includes Spring Data JPA for ORM capabilities.
[^4]: AccountQueryForm.java: `buildPredicate(Group group)` - Uses QueryDSL's `QAccount` and `BooleanBuilder` to dynamically construct a database query.
[^5]: BaseRepository.java: `BaseRepository<T extends BaseEntity>` - Defines a common repository interface with methods for CRUD operations and custom queries.
[^6]: SqlScriptRunner.java: `run(ApplicationArguments args)` - Implements an `ApplicationRunner` to execute `1.sql` at startup using `JdbcTemplate`.
[^7]: .github/workflows/docker-image-ali.yml: `registry: registry.cn-hangzhou.aliyuncs.com` - Specifies the Aliyun Container Registry for Docker image storage.
[^8]: AccountService.java: `add(AccountAddForm form)` - Logic for creating and saving a new `Account` entity.
[^9]: BalanceFlowService.java: `query(BalanceFlowQueryForm request, Pageable page)` - Retrieves a paginated list of transactions from the database.
[^10]: BalanceFlowService.java: `confirmBalance(BalanceFlow flow)` - Contains the logic to update account balances and confirm a transaction within a single method.
[^11]: ReportService.java: `reportCategory(CategoryReportQueryForm form, CategoryType type)` - Gathers data from multiple entities and relations to create an aggregate report.
[^12]: AccountController.java: `handleQuery(AccountQueryForm form, Pageable page)` - Controller method that accepts a `Pageable` object to handle pagination.
[^13]: Account.java: `@ManyToOne(optional = false, fetch = FetchType.LAZY)` - Example of lazy loading configured for an entity relationship.
[^14]: application.properties: `spring.jpa.properties.hibernate.jdbc.batch_size=50` - Configures Hibernate to batch write operations.
[^15]: CurrencyDataLoader.java: `run(ApplicationArguments args)` - Loads currency data into an `ApplicationScopeBean` at startup.

### Data Access and Management
The application employs a data-centric architecture where the persistence layer is a cornerstone of its functionality. This section examines the mechanisms for data access, manipulation, and management, providing a clear picture of the data lifecycle within the system. The analysis highlights the use of standard Java Persistence frameworks and identifies key patterns that are critical for understanding the system's current state and planning for future modernization.

#### Data Access Patterns

The application consistently uses a repository pattern facilitated by Spring Data JPA and Hibernate for its data access needs. Complex queries are constructed dynamically using QueryDSL, providing a type-safe approach to database interaction.

##### Objective-Relational Mapping (ORM)

The application leverages Spring Data JPA with Hibernate as the persistence provider. Data is modeled as Plain Old Java Objects (POJOs) annotated with Jakarta Persistence (`@Entity`) annotations. These entities are mapped to tables in a MySQL database [^1]. This approach abstracts direct SQL interaction, allowing developers to work with Java objects.

The core entities managed by the ORM include:

| Entity | Path | Description |
| :--- | :--- | :--- |
| `Account` | `cn/biq/mn/account/Account.java` | Represents user financial accounts (e.g., checking, credit). |
| `Book` | `cn/biq/mn/book/Book.java` | Represents a ledger for organizing transactions. |
| `BalanceFlow` | `cn/biq/mn/balanceflow/BalanceFlow.java` | The central entity for all transactions (expense, income, transfer). |
| `Category` | `cn/biq/mn/category/Category.java` | Defines hierarchical categories for transactions. |
| `Tag` | `cn/biq/mn/tag/Tag.java` | Defines hierarchical tags for transactions. |
| `Payee` | `cn/biq/mn/payee/Payee.java` | Represents the recipient or source of a transaction. |
| `User` | `cn/biq/mn/user/User.java` | Represents an application user. |
| `Group` | `cn/biq/mn/group/Group.java` | Manages multi-tenancy and user collaboration. |

##### CRUD Operations

Standard Create, Read, Update, and Delete (CRUD) operations are implemented through the Spring Data `JpaRepository` interface. A custom `BaseRepository` interface extends these capabilities, which is then implemented by specific entity repositories like `AccountRepository` and `BookRepository` [^2].

- **Create**: New entities are persisted using the `repository.save()` method, typically within a service method like `AccountService.add()` [^3].
- **Read**: Data retrieval is handled via methods like `repository.findById()`, `repository.findAll()`, and dynamic queries built with QueryDSL.
- **Update**: Updates are also performed using `repository.save()`. In a transactional context, modifying an entity retrieved from the database and saving it will trigger an update.
- **Delete**: Entities are removed using `repository.delete()`, as seen in methods like `AccountService.remove()` [^4]. These operations often include pre-deletion checks to maintain data integrity.

##### Querying

The application employs a combination of Spring Data JPA and QueryDSL for data querying to balance simplicity and power.

*   **Spring Data JPA**: For simple queries, the application relies on the built-in methods of `JpaRepository` (e.g., `findById`, `existsBy...`).
*   **QueryDSL**: For more complex, dynamic queries with multiple optional filters, the system uses QueryDSL. `QueryForm` objects (e.g., `AccountQueryForm`) contain the logic to build a `Predicate` dynamically based on request parameters. This pattern provides compile-time safety and maintainability for complex search functionalities [^5].

```java
// Example from AccountQueryForm.java
public Predicate buildPredicate(Group group) {
    QAccount account = QAccount.account;
    BooleanBuilder expression = new BooleanBuilder(account.group.eq(group));
    if (enable != null) {
       expression.and(account.enable.eq(enable));
    }
    if (type != null) {
        expression.and(account.type.eq(type));
    }
    // ... more conditions
    return expression;
}
```

*   **Aggregations**: Aggregate functions like `SUM` are implemented in a custom repository base class, `BaseRepositoryImpl`, which uses a `JPAQueryFactory` to execute aggregate queries [^6]. This is heavily used in the reporting features.

##### Data Transformation

Data is consistently transformed between layers using a `Mapper` pattern. These mapper classes are responsible for converting data between different representations:
*   **Entity to DTO**: `Mapper` classes convert persistence entities (`@Entity`) into Data Transfer Objects (`Details` classes) for API responses. This ensures that only necessary and safe-to-expose data is sent to the client [^7].
*   **DTO to Entity**: They also map incoming data from request bodies (`Form` classes) into new or existing entity objects before they are persisted [^8].

```java
// Example from AccountMapper.java
public static AccountDetails toDetails(Account entity) {
    if (entity == null) return null;
    var details = new AccountDetails();
    details.setId( entity.getId() );
    details.setName( entity.getName() );
    // ... mapping other fields
    return details;
}
```

##### Transaction Analysis

The application uses Spring's declarative transaction management via the `@Transactional` annotation on service-layer methods. This ensures that operations are atomic and maintain data consistency. Most business logic is encapsulated within these transactional boundaries.

| Transaction (Method) | Type | Complexity | Involved Entities/Tables |
| :--- | :--- | :--- | :--- |
| `UserService.register` | Read-Write | High | `User`, `Group`, `Book`, `UserGroupRelation`, `Category`, `Tag`, `Payee` |
| `BalanceFlowService.add` | Read-Write | High | `BalanceFlow`, `Account`, `Book`, `CategoryRelation`, `TagRelation`, `Payee` |
| `AccountService.add` | Read-Write | Medium | `Account`, `Group` |
| `BookService.remove` | Read-Write | Medium | `Book`, `BalanceFlow`, `Category`, `Payee`, `Tag` |
| `AccountService.query` | Read-Only | Low | `Account`, `Currency` (via service call) |
| `ReportService.reportCategory` | Read-Only | High | `CategoryRelation`, `BalanceFlow`, `Category`, `Book` |

#### Data Management

Data management practices focus on ensuring integrity, security through access control, and providing basic mechanisms for backup.

##### Data Integrity

Data integrity is enforced at multiple levels:
1.  **Database Constraints**: The ORM entities use annotations like `@Column(nullable = false)` and `@UniqueConstraint` to define schema-level rules.
2.  **Application-Level Validation**: Jakarta Bean Validation annotations (`@NotNull`, `@Size`, etc.) and custom validation annotations (e.g., `@NameField`) are used on `Form` objects to validate incoming data before processing [^9].
3.  **Transactional Consistency**: As described above, `@Transactional` annotations ensure that multi-step operations either complete successfully or are fully rolled back.
4.  **Exception Handling**: A `GlobalExceptionHandler` catches `DataIntegrityViolationException`, preventing application crashes from database errors and providing a coherent error response to the client [^10].

##### Data Governance

The application enforces data governance primarily through a multi-tenancy model centered around the `Group` entity.
- **Data Scoping**: Nearly all data access is scoped to the current user's active `Group`. Service methods consistently retrieve the current group from the user's session and use it to filter database queries [^11]. This ensures that users can only access data belonging to the groups they are members of.
- **Role-Based Access**: Within a group, user access is further defined by roles (`Owner`, `Operator`, `Guest`). These roles are checked in service methods like `GroupService.update` to authorize actions [^12].

##### Data Backup and Recovery

The current data backup and recovery strategy is manual and documented in `notes/data_backup.txt` [^13]. It outlines two primary methods:
1.  **phpMyAdmin Export/Import**: Using a database management tool to perform a logical backup of the `moneynote` database.
2.  **Filesystem Backup**: Directly backing up the MySQL data directory (`/var/lib/mysql/moneynote`) from the Docker container volume. The document provides shell commands for this process.

Additionally, the application offers a feature to export transaction data from a `Book` into an Excel (`.xlsx`) file, which serves as a user-managed form of data backup [^14].

#### Key Challenges and Limitations

The current data management practices present several challenges that should be addressed during modernization:

*   **Monolithic Database**: The entire application relies on a single MySQL database instance. As the application scales, this can become a performance bottleneck and a single point of failure. Modernization should consider database strategies like read replicas or sharding.
*   **Manual Backup and Recovery**: The backup process is entirely manual, error-prone, and requires direct access to the server environment. This is not a scalable or reliable strategy for a production system. Migrating to a managed database service on Google Cloud, such as Google Cloud SQL, would provide automated, point-in-time recovery and scheduled backups, significantly improving reliability.
*   **Rudimentary Schema Migration**: Schema updates are handled by a custom SQL script runner that executes `1.sql` on startup [^15]. This approach lacks the versioning, rollback capabilities, and robustness of dedicated database migration tools like Flyway or Liquibase. Adopting such a tool would make schema changes safer and more manageable.
*   **Reporting on a Transactional Database**: Complex analytical queries for reporting are run directly against the primary transactional database [^16]. This can negatively impact application performance. For a cloud-native architecture, it would be beneficial to replicate data to a dedicated data warehouse like Google Cloud BigQuery for analytics and reporting, isolating workloads and improving performance for both transactional and analytical tasks.

[^1]: src/main/resources/application.properties: spring.datasource.url - Defines the JDBC connection string for a MySQL database.
[^2]: src/main/java/cn/biq/mn/base/BaseRepository.java: BaseRepository interface - Extends JpaRepository and QuerydslPredicateExecutor to provide a common data access contract.
[^3]: src/main/java/cn/biq/mn/account/AccountService.java: add() - Persists a new Account entity after validation and mapping.
[^4]: src/main/java/cn/biq/mn/account/AccountService.java: remove() - Deletes an account after checking for associated data.
[^5]: src/main/java/cn/biq/mn/account/AccountQueryForm.java: buildPredicate() - Uses QueryDSL's BooleanBuilder to construct dynamic queries for accounts.
[^6]: src/main/java/cn/biq/mn/base/BaseRepositoryImpl.java: calcSum() - Implements a generic method to calculate the sum of a BigDecimal column using JPAQueryFactory.
[^7]: src/main/java/cn/biq/mn/account/AccountMapper.java: toDetails() - Maps an `Account` entity to an `AccountDetails` DTO.
[^8]: src/main/java/cn/biq/mn/account/AccountMapper.java: toEntity() - Maps an `AccountAddForm` DTO to an `Account` entity.
[^9]: src/main/java/cn/biq/mn/account/AccountAddForm.java: @NameField - Custom validation annotation applied to form fields.
[^10]: src/main/java/cn/biq/mn/exception/GlobalExceptionHandler.java: exceptionHandler(DataIntegrityViolationException e) - Handles database constraint violations.
[^11]: src/main/java/cn/biq/mn/base/BaseService.java: findAccountById() - Ensures that the retrieved account belongs to the current user's group.
[^12]: src/main/java/cn/biq/mn/group/GroupService.java: checkRole() - Validates if a user has the owner role within a group before allowing modifications.
[^13]: notes/data_backup.txt: data_backup.txt - Contains manual instructions for backing up the MySQL database via phpmyadmin or Docker volume copy.
[^14]: src/main/java/cn/biq/mn/book/BookService.java: exportFlow() - Exports all balance flow records of a book into an XLSX file.
[^15]: src/main/java/cn/biq/mn/SqlScriptRunner.java: run() - Executes the `1.sql` script upon application startup to perform database schema updates.
[^16]: src/main/java/cn/biq/mn/report/ReportService.java: reportCategory() - Executes potentially complex aggregation queries directly on the transactional database for reporting purposes.

### Security Considerations
#### Data Protection

The application employs several mechanisms to protect data, primarily through password hashing and a structured access control model. However, there are significant gaps in data encryption, both at rest and in transit.

##### Encryption

Data encryption is a critical component of security, but its implementation in the current system is limited.

| Encryption Type | Analysis | Recommendations |
| :--- | :--- | :--- |
| **Data in Transit** | The application does not enforce HTTPS for API endpoints within its own configuration. The database connection string also lacks SSL/TLS parameters, meaning data transfer between the application and the MySQL database is likely unencrypted. | Implement TLS termination at a load balancer or reverse proxy. Enforce HTTPS for all API communication. Configure the database connection to use SSL/TLS to protect data in transit between the application and the database server. |
| **Data at Rest (Passwords)** | User passwords are not stored in plaintext. The system uses the BCrypt algorithm to hash passwords upon user registration and for authentication checks, which is a strong, industry-standard practice [^1]. This is configured via the `PasswordEncoder` bean [^2]. | Continue using a strong, adaptive hashing algorithm like BCrypt for password storage. |
| **Data at Rest (Sensitive Data)** | Beyond password hashing, there is no application-level encryption for other sensitive data stored in the database, such as account numbers (`no` field in the `Account` entity) or personal notes. | For highly sensitive data, consider implementing application-level field encryption. Cloud-native solutions like Google Cloud's Key Management Service can be leveraged to manage encryption keys securely during modernization. |

##### Access Control

Access control is managed through a combination of JWT-based authentication and an application-level, role-based authorization model tied to user groups.

###### Authentication

Authentication is handled via a traditional username/password login flow that generates a JSON Web Token (JWT) upon success [^3]. This token is then used to authenticate subsequent API requests.

1.  **Login Process**: The `UserService.login()` method validates user credentials against the hashed password stored in the database.
2.  **Token Generation**: Upon successful validation, a JWT is created by `JwtUtils.createToken()` [^4]. This token contains the user's ID and has an expiration of 30 days.
3.  **Token Validation**: The `AuthInterceptor` intercepts all incoming requests (except for public endpoints like `/login` and `/register`). It validates the JWT from the `Authorization: Bearer` header and populates a session-scoped `CurrentSession` bean with the authenticated user's context [^5].

###### Authorization

Authorization is enforced at the application layer and is centered around `Group` membership and assigned roles. This model ensures that users can only access data within the groups they belong to.

*   **Data Scoping**: Most data access logic ensures that queries are filtered by the current user's active group. For instance, methods in `BaseService` consistently check if a requested entity (e.g., an `Account` or `BalanceFlow`) belongs to the user's current group before returning it [^6]. Query forms also build predicates that enforce this data segregation [^7].
*   **Role-Based Access Control (RBAC)**: The `UserGroupRelation` entity defines user roles within a group using integer codes. These roles dictate the actions a user can perform.

| Role ID | Role Name | Permissions & Responsibilities |
| :--- | :--- | :--- |
| 1 | Owner | Full control over the group, including inviting/removing users, deleting the group, and modifying its settings. The `GroupService` checks for this role for administrative actions [^8]. |
| 2 | Operator | Can manage data within the group but cannot perform administrative actions like deleting the group or removing other users. |
| 3 | Guest | Read-only access to group data. |
| 4 | Invited | A temporary status for users who have been invited to a group but have not yet accepted. They have no access until they agree to the invite. |

##### Data Masking and Anonymization

The codebase does not contain any explicit mechanisms for data masking or anonymization. Sensitive information, once retrieved from the database, is presented as-is in API responses. While access is restricted, the data itself is not obscured.

#### Vulnerability Assessment

The application has several security vulnerabilities, ranging from insecure configuration to potential injection risks, that need to be addressed.

##### Injection Vulnerabilities

The application primarily uses Spring Data JPA with QueryDSL for database interactions, which provides strong protection against SQL injection. Dynamic queries are constructed using `BooleanBuilder`, which relies on parameterized queries, effectively mitigating this risk [^7].

However, the `SqlScriptRunner` component executes a raw SQL script (`1.sql`) at startup by splitting the file content on semicolons [^9]. While the current script is static, this implementation is fragile and could be a vulnerability if the script's source were ever made dynamic. The splitting logic can also fail if SQL statements contain semicolons within string literals.

##### Authentication and Session Management

The most significant security flaw lies in the management of the JWT secret key.

*   **Hardcoded Secret Key**: The secret key for signing and verifying JWTs is hardcoded directly into the `JwtUtils` class [^4]. This is a critical vulnerability. Anyone with access to the source code or the compiled JAR file can extract this key, allowing them to forge valid JWTs for any user and gain unauthorized access to the application.
*   **Hybrid Session Model**: The application uses a hybrid approach, combining stateless JWTs with stateful server-side sessions (`@SessionScope`, `HttpServletRequest.getSession()`). The `AuthInterceptor` populates a session-scoped bean [^5], and the `UserController` directly interacts with the `HttpServletRequest` session [^10]. This model introduces unnecessary complexity, hurts scalability, and deviates from the common practice of using purely stateless tokens for REST APIs.

##### Access Control Vulnerabilities

The access control model is generally well-implemented through data scoping. However, the security of the entire model is undermined by the hardcoded JWT secret. If an attacker can forge a token, they can impersonate any user and bypass the intended access controls.

#### Compliance

The application was not built with any specific data privacy or security compliance frameworks (e.g., GDPR, CCPA) in mind.

*   **Right to Erasure**: The system supports hard deletes of data (e.g., `AccountService.remove()`), which could be used to fulfill a "right to be forgotten" request. However, there is no formalized process for this.
*   **Audit Logging**: There is no dedicated audit trail to log access to sensitive data or critical actions performed by users. This is a major gap for compliance and for incident response.

#### Key Challenges and Recommendations

The current security posture presents several challenges that must be addressed before or during the migration to Google Cloud.

| Category | Finding | Recommendation | Priority |
| :--- | :--- | :--- | :--- |
| **Authentication** | **Hardcoded JWT secret key** in `JwtUtils.java`. | Externalize the secret key. Use a secure secret management service like Google Cloud Secret Manager. Do not store secrets in source code or configuration files. | **Critical** |
| **Configuration** | Default database password present in `application.properties`. | Remove the default password. Rely exclusively on environment variables or a secret management system for database credentials. | **High** |
| **Encryption** | Lack of enforced TLS for API endpoints and the database connection. | Enforce HTTPS for all traffic using a load balancer. Configure the JDBC connection to use SSL, ensuring data is encrypted in transit. | **High** |
| **Architecture** | Hybrid stateful session and stateless JWT model. | Refactor to a purely stateless JWT authentication model. Remove dependencies on `@SessionScope` and `HttpServletRequest.getSession()` for authentication context. | **Medium** |
| **Logging** | No security or audit logging is implemented. | Integrate a structured logging framework and implement comprehensive audit logging for security-relevant events (logins, data modification, permission changes). | **Medium** |
| **Code Quality** | Brittle SQL execution logic in `SqlScriptRunner`. | If startup scripts are necessary, replace the manual SQL runner with a robust database migration tool like Flyway or Liquibase. | **Low** |

[^1]: src/main/java/cn/biq/mn/user/UserService.java: register() - Encodes the user's password using `passwordEncoder.encode()` before saving it.
[^2]: src/main/java/cn/biq/mn/security/SecurityConfig.java: passwordEncoder() - Defines the `PasswordEncoder` bean as a `BCryptPasswordEncoder`.
[^3]: src/main/java/cn/biq/mn/user/UserService.java: login() - Authenticates users by comparing the provided password with the stored hash using `passwordEncoder.matches()`.
[^4]: src/main/java/cn/biq/mn/security/JwtUtils.java: createToken(), getUserId() - Contains logic for creating and verifying JWTs, but uses a hardcoded secret key `private final String secretKey = "******";`.
[^5]: src/main/java/cn/biq/mn/interceptor/AuthInterceptor.java: preHandle() - Intercepts requests, validates the JWT, and populates the session-scoped `CurrentSession` bean.
[^6]: src/main/java/cn/biq/mn/user/UserGroupRelation.java: UserGroupRelation - Defines the relationship between a user and a group, including a `role` field.
[^7]: src/main/java/cn/biq/mn/account/AccountQueryForm.java: buildPredicate() - Uses QueryDSL's `BooleanBuilder` to safely construct dynamic queries based on user input, preventing SQL injection.
[^8]: src/main/java/cn/biq/mn/group/GroupService.java: checkRole() - A private method that verifies if the current user has the role of 'Owner' (role ID 1) in a group.
[^9]: src/main/java/cn/biq/mn/SqlScriptRunner.java: run() - Executes a raw SQL script from a classpath resource upon application startup.
[^10]: src/main/java/cn/biq/mn/user/UserController.java: handleLogin() - Directly interacts with `HttpServletRequest.getSession()` to store the access token.
[^11]: src/main/java/cn/biq/mn/base/BaseService.java: findFlowById() - Ensures a requested `BalanceFlow` entity belongs to the currently authenticated user's group, preventing insecure direct object reference.

## Assessment Findings
### Modernization Opportunities
#### Database Modernization

The application currently utilizes a MySQL database, as indicated by its JDBC driver configuration and connection string [^1]. Data access is managed through Spring Data JPA with Hibernate, which provides a solid foundation but also presents opportunities for modernization by leveraging managed cloud database services.

A migration to a managed database service on Google Cloud would offload significant operational burdens such as patching, backups, and availability management, allowing the development team to focus on application features.

| Service | Benefits | Challenges | Best Fit Scenario |
| :--- | :--- | :--- | :--- |
| **Google Cloud SQL for MySQL** | • **Low Migration Effort**: Fully compatible with the existing MySQL setup. Requires minimal code changes, primarily updating the JDBC connection string.<br>• **Fully Managed**: Automates backups, replication, patches, and updates.<br>• **Cost-Effective**: Offers flexible pricing and the ability to right-size instances to match workload demands.<br>• **Improved Performance**: Provides options for high-availability configurations and read replicas to scale read-heavy workloads. | • **Scalability Limits**: Primarily scales vertically. While sufficient for many workloads, it may not meet the needs of a hyper-scale global application. | For a direct, low-risk migration that immediately reduces operational overhead while retaining the existing application architecture and codebase. Ideal for the current scale of the MoneyNote application. |
| **Google Cloud Spanner** | • **Global Scale & Consistency**: Provides horizontal scalability with strong consistency across regions, suitable for a globally distributed user base.<br>• **High Availability**: Offers a 99.999% availability SLA, far exceeding what is typical for traditional relational databases.<br>• **Transactional Consistency**: Maintains ACID properties at a global scale. | • **High Migration Effort**: Requires significant schema redesign and application code changes. Spanner is not a drop-in replacement for MySQL, and the existing JPA/Hibernate implementation would need to be re-evaluated.<br>• **Higher Cost at Small Scale**: Can be more expensive than Cloud SQL for smaller workloads. | For a long-term strategic evolution of the application into a large-scale, multi-tenant SaaS platform where global distribution and extreme availability are critical business requirements. |

##### Recommendation

For an initial modernization step, migrating the existing MySQL database to **Google Cloud SQL for MySQL** is the most practical and beneficial approach. It offers a clear path to reducing operational costs and improving reliability with minimal disruption to the current application. The potential for using Google Cloud Spanner can be revisited in the future if the application's user base grows to a global scale.

#### Storage Modernization

The application currently stores user-uploaded files, such as attachments to financial records, directly within the MySQL database as `LONGBLOB` data types [^2]. This approach can lead to performance degradation, increased database size, and higher storage costs. Modernizing this layer involves offloading this binary data to a dedicated object storage service.

**Google Cloud Storage** is an ideal solution for this purpose. By migrating file storage, the application can achieve significant improvements in scalability, cost, and performance.

##### Modernization Strategy

1.  **Migrate Data**: Move existing `LONGBLOB` data from the database to a Google Cloud Storage bucket.
2.  **Update Application Logic**:
    *   Modify the `FlowFile` entity to store a reference to the object in Cloud Storage (e.g., its path or name) instead of the `byte[]` data.
    *   Update the `BalanceFlowService` to upload new files directly to the Cloud Storage bucket [^3].
    *   Change the file retrieval logic (`FlowFileController`) to generate secure, time-limited signed URLs for clients to download files directly from Cloud Storage, reducing the load on the application backend [^4].

| Aspect | Current Approach (Database `LONGBLOB`) | Proposed Approach (Google Cloud Storage) |
| :--- | :--- | :--- |
| **Scalability** | Limited by database capacity and performance. | Virtually limitless, designed for exabyte-scale data. |
| **Cost** | High cost associated with database storage, which is optimized for transactional data, not blobs. | Significantly lower cost per GB. Supports storage classes (e.g., Standard, Nearline) to further optimize costs based on access frequency. |
| **Performance** | Slows down database queries and backups. Increases application server load for file streaming. | Offloads file serving from the database and application. Enables direct client access and integration with Google Cloud CDN for low-latency delivery. |
| **Durability** | Dependent on database backup and replication strategy. | Extremely high durability with geo-redundancy options. |

#### Data Processing and Analytics

The application features a reporting module that generates analytics on expenses, income, tags, and payees. These analytical queries are executed directly against the production OLTP database [^5]. While functional at a small scale, this architecture can lead to performance contention as the data volume and query complexity grow, impacting the primary application's responsiveness.

A modern approach involves separating transactional workloads (OLTP) from analytical workloads (OLAP) using a dedicated data warehouse.

##### Proposed Analytics Architecture

1.  **Data Warehouse**: Use **Google BigQuery** as the central data warehouse. Its serverless architecture, powerful query engine, and cost-effective pricing make it ideal for analytics.
2.  **ETL Pipeline**: Implement a data pipeline using **Google Cloud Dataflow** to extract data from the production database (running on Cloud SQL), transform it into an analytics-friendly format, and load it into BigQuery. This process can be run in batch mode (e.g., nightly) or as a streaming pipeline for real-time analytics.
3.  **Reporting Service**: Refactor the `ReportService` to query BigQuery for all analytical reports. This decouples reporting from the transactional database, ensuring that complex analytical queries do not affect application performance.

##### Benefits of Modernization

| Component | Benefit |
| :--- | :--- |
| **Google BigQuery** | • **Scalability and Performance**: Massively parallel query execution for high-speed analytics on large datasets.<br>• **Serverless**: No infrastructure to manage, reducing operational overhead.<br>• **Cost-Effective**: Pay-per-query model allows for cost control. |
| **Google Cloud Dataflow** | • **Managed ETL**: Fully managed service for building reliable and scalable batch and streaming data pipelines.<br>• **Unified Model**: Supports both batch and streaming sources with the same code.<br>• **Automatic Scaling**: Manages compute resources automatically based on workload. |

#### Caching and Performance Optimization

The application employs an in-memory cache using an `ApplicationScopeBean` for currency rates and book templates [^6]. This local cache is not suitable for a distributed environment, as it would lead to data inconsistencies and stale data across different application instances. Furthermore, user session data is managed via the standard HTTP session, which hinders stateless scaling [^7].

##### Caching Strategy

| Opportunity | Recommended Service | Implementation Details |
| :--- | :--- | :--- |
| **Distributed Caching** | **Google Cloud Memorystore for Redis** | Replace the `ApplicationScopeBean` with a Redis-backed cache. Store frequently accessed, semi-static data like currency rates, user permissions, and book templates. This ensures data consistency across all application instances and reduces load on the database. |
| **Session Management** | **Google Cloud Memorystore for Redis** | Externalize HTTP session state to Redis. This allows the application to become stateless, enabling seamless horizontal scaling and improving resilience, as application instances can be terminated and replaced without user session loss. |
| **Content Delivery** | **Google Cloud CDN** | If static assets (e.g., images, uploaded files in Cloud Storage) are served, enable Cloud CDN to cache this content at the edge of Google's network. This dramatically reduces latency for global users and lowers data egress costs. |

#### Security and Compliance

The application handles sensitive financial data, making security a paramount concern. Migrating to Google Cloud provides access to a suite of advanced security services that can significantly enhance the application's security posture.

| Security Layer | Google Cloud Service | Application in MoneyNote |
| :--- | :--- | :--- |
| **Access Control** | **Identity and Access Management (IAM)** | Define granular permissions for the application's service account to ensure it has only the necessary access to Cloud SQL databases and Cloud Storage buckets (principle of least privilege). |
| **Network Security** | **VPC Service Controls** | Create a service perimeter around the project's Google Cloud resources to prevent data exfiltration. This ensures that sensitive data within Cloud SQL and Cloud Storage cannot be accessed from outside the trusted network boundary. |
| **Web Application Security** | **Google Cloud Armor** | Deploy Cloud Armor with the Google Cloud Load Balancer to protect the API from DDoS attacks and common web vulnerabilities (e.g., SQL injection, XSS) via its pre-configured WAF rules. |
| **Data Protection** | **Cloud Data Loss Prevention (DLP)** | Periodically scan data stored in Cloud Storage buckets or the database to discover, classify, and protect sensitive user information, aiding in compliance with regulations like GDPR. |

#### Cost Optimization

Migrating to Google Cloud and adopting cloud-native patterns presents several opportunities for cost optimization by moving from a provisioned-capacity model to a pay-per-use model.

| Area | Opportunity | Benefit |
| :--- | :--- | :--- |
| **Compute** | Deploy the application as a container on **Google Cloud Run**. The project already includes a `Dockerfile` and CI/CD pipelines for building container images, which simplifies this transition [^8]. | **Pay-per-use pricing**: Cloud Run scales down to zero when there are no requests, eliminating costs for idle resources. This is ideal for applications with variable or unpredictable traffic patterns.<br>**Reduced operational overhead**: As a fully managed serverless platform, Cloud Run handles all infrastructure management. |
| **Storage** | Migrate file storage from the database to **Google Cloud Storage** and utilize **Storage Lifecycle Management**. | **Lower storage costs**: Cloud Storage is significantly cheaper than database storage for blobs.<br>**Automated cost savings**: Automatically transition older, less-frequently accessed files (e.g., attachments from previous years) to cheaper storage classes like Nearline or Coldline. |
| **Database** | Use **Google Cloud SQL's** flexible instance sizing and automated scaling. | **Right-sizing**: Avoid over-provisioning by selecting an instance size that matches the current workload and easily scaling up as needed.<br>**Operational savings**: Eliminates the costs associated with database administration tasks like backups, patching, and maintenance. |
| **Asynchronous Tasks** | Offload event-driven or background tasks to **Google Cloud Functions**. For example, a file upload to Cloud Storage could trigger a Cloud Function to generate a thumbnail or perform validation [^3]. | **Serverless execution**: Pay only for the execution time of the function, measured in milliseconds. This is highly cost-effective for sporadic or infrequent tasks compared to having a server constantly running to listen for events. |

[^1]: src/main/resources/application.properties: spring.datasource.url - Defines the JDBC connection string for a MySQL database.
[^2]: src/main/java/cn/biq/mn/flowfile/FlowFile.java: data - A `byte[]` field mapped to a `LONGBLOB` column for storing file data directly in the database.
[^3]: src/main/java/cn/biq/mn/balanceflow/BalanceFlowService.java: addFile - Method responsible for handling file uploads and associating them with a balance flow record.
[^4]: src/main/java/cn/biq/mn/flowfile/FlowFileController.java: handleView - Endpoint that serves file data from the backend.
[^5]: src/main/java/cn/biq/mn/report/ReportService.java: reportCategory - An example of a reporting method that executes aggregation queries directly against the transactional database.
[^6]: src/main/java/cn/biq/mn/bean/ApplicationScopeBean.java: ApplicationScopeBean - A Spring bean with application scope used as an in-memory cache for currencies and book templates.
[^7]: src/main/java/cn/biq/mn/security/CurrentSession.java: CurrentSession - A session-scoped bean that holds user state, tied to the underlying HTTP session.
[^8]: .github/workflows/docker-image.yml: docker/build-push-action - GitHub Actions workflow that builds and pushes a Docker image, demonstrating containerization readiness.

### Modernization Challenges
Modernizing the data layer of the MoneyNote application presents a series of challenges that require careful planning and execution. A successful migration to Google Cloud involves more than a simple lift-and-shift of the database; it necessitates a thorough analysis of the existing data architecture, application dependencies, and operational processes. This section outlines the primary challenges and potential roadblocks in data migration, application compatibility, security, cost management, and team skills.

#### 1. Data Migration

Migrating the existing MySQL database to a managed service like Google Cloud SQL for MySQL is a critical step. However, this process is fraught with challenges that could impact data integrity, availability, and performance.

##### Migration Complexity

The primary challenge lies in the nature of the existing data and the documented backup procedures. The database schema appears to be managed by Spring Data JPA, which simplifies schema understanding [^1]. However, the presence of a manual SQL script for schema evolution indicates that migrations are not fully automated through a dedicated tool, adding a layer of risk [^2].

A significant concern is the backup and restore procedure described in the project notes. This process relies on manual `tar` commands and suggests converting tables to the MyISAM storage engine for easier file-based transfers [^3]. This approach is highly brittle, error-prone, and unsuitable for a production environment.

| Challenge | Risk | Recommended Action |
| :--- | :--- | :--- |
| **Manual Schema Scripts** | Incomplete or incorrect application of scripts during migration could lead to data corruption or application errors. | Analyze all historical SQL scripts and consolidate them. Use the Google Cloud Database Migration Service, which can handle schema conversion and validation. |
| **MyISAM Storage Engine** | MyISAM lacks support for transactions and foreign key constraints, increasing the risk of data inconsistency. It is also not fully supported by most managed database services, which default to InnoDB. | Before migration, convert all MyISAM tables to InnoDB to ensure transactional integrity and compatibility with Google Cloud SQL. |
| **Data Consistency** | Financial data requires absolute consistency. A manual, file-based migration process carries a high risk of data loss or inconsistent states between related tables. | Utilize the Google Cloud Database Migration Service for a continuous, low-downtime migration that ensures data is validated and consistent between the source and target databases. |
| **Downtime** | As a user-facing application, any significant downtime during migration will negatively impact the user experience. | Plan for a minimal downtime migration. Use a continuous replication strategy where the Google Cloud SQL instance is kept in sync with the source database, allowing for a quick cutover. |

#### 2. Application Compatibility

While the application is built on a modern framework (Spring Boot), specific implementation choices present compatibility and modernization challenges.

##### Database-Centric File Storage

The application currently stores user-uploaded files, such as receipt images, directly within the MySQL database as `LONGBLOB` data types [^4]. This approach has several drawbacks:

*   **Performance Degradation**: Storing large binary objects in the database can bloat tables, slow down queries, and increase the time required for backups and restores.
*   **Scalability Issues**: Database storage is significantly more expensive than dedicated object storage. As the number of users and transactions grows, this will lead to escalating costs and performance bottlenecks.
*   **Refactoring Effort**: Moving this data to a cloud-native solution requires code changes. The `FlowFileService`, which handles file operations, would need to be refactored to interact with a service like Google Cloud Storage instead of the local database repository [^5].

| Current Implementation | Proposed Google Cloud Solution | Modernization Benefit |
| :--- | :--- | :--- |
| `FlowFile` entity with `byte[] data` stored in MySQL. | Store files in a **Google Cloud Storage** bucket. | **Cost Reduction**: Object storage is more cost-effective for binary data. |
| Files retrieved via database queries. | The database stores only a URL or reference to the file in Google Cloud Storage. | **Improved Performance**: Offloads large object retrieval from the database, freeing it up for transactional workloads. |
| Files are part of database backups. | Files are managed by Google Cloud Storage lifecycle policies. | **Enhanced Scalability & Durability**: Google Cloud Storage is designed for massive scale and provides higher data durability. |

#### 3. Security and Compliance

Migrating sensitive financial data to the cloud introduces new security and compliance considerations that must be addressed proactively.

##### Data Residency and Sovereignty

The CI/CD pipeline configurations indicate that container images are pushed to Alibaba Cloud's container registry in the `cn-hangzhou` region [^6]. This suggests the application may currently serve users in China or have ties to infrastructure in that region. When migrating to Google Cloud, data residency becomes a critical concern. Personal financial data is often subject to strict regulations (e.g., GDPR, CCPA) that dictate where data can be stored and how it is processed.

**Key Challenges**:

*   **Regulatory Adherence**: Ensuring the chosen Google Cloud region complies with all relevant data protection laws for the application's user base.
*   **Cross-Border Data Transfer**: If the user base is global, policies for managing cross-border data transfers must be established.
*   **User Trust**: Users of a financial application expect their data to be handled with the utmost care. Clear communication about data storage policies will be essential for maintaining trust.

##### Securing Credentials and Data

The application uses modern practices for password hashing, employing `BCryptPasswordEncoder` [^7]. However, the management of database credentials in the configuration file presents a security risk. Although it supports environment variables, a hardcoded fallback password is a potential vulnerability [^8].

| Area | Challenge | Recommendation |
| :--- | :--- | :--- |
| **Database Credentials** | Storing database passwords in configuration files or environment variables is a security risk. | Integrate with **Google Cloud Secret Manager** to securely store and manage all application secrets, including database credentials and API keys. Update the application to fetch credentials from Secret Manager at runtime. |
| **Data Encryption** | Data must be secure both in transit and at rest. | Enforce SSL/TLS for all connections to Google Cloud SQL. Leverage Google Cloud's default encryption-at-rest capabilities for both Cloud SQL and Google Cloud Storage. |
| **Access Control** | Granting broad access permissions to the database increases the risk of unauthorized access. | Implement the principle of least privilege using **Google Cloud Identity and Access Management (IAM)**. Create fine-grained service accounts for the application with limited permissions. |

#### 4. Cost Management

While modernization can lead to long-term cost savings, the migration project itself and the subsequent cloud resource consumption must be carefully managed.

##### Predicting and Optimizing Cloud Spend

The shift from a potentially fixed-cost infrastructure to a pay-as-you-go cloud model requires a new approach to financial governance.

*   **Right-Sizing Resources**: The initial selection of a Google Cloud SQL instance size is a critical cost lever. Over-provisioning will lead to unnecessary expenses, while under-provisioning will cause performance issues. A thorough performance analysis of the current database is needed to make an informed decision.
*   **Storage Costs**: As discussed, migrating `LONGBLOB` data to Google Cloud Storage is a significant cost-optimization opportunity. Implementing lifecycle policies in Google Cloud Storage to move older, infrequently accessed files to cheaper storage tiers (e.g., Nearline or Coldline) can further reduce costs.
*   **Operational Expenses**: While managed services like Google Cloud SQL reduce the operational burden of tasks like patching and backups, there will be new costs associated with monitoring, logging, and alerts through services like Google Cloud's operations suite. These should be factored into the total cost of ownership (TCO) analysis.

#### 5. Organizational and Technical Skills

A successful modernization effort depends heavily on the skills and readiness of the development team.

##### Bridging the Skill Gap

The team demonstrates strong proficiency in Java, Spring Boot, and Docker, which provides a solid foundation. However, the manual backup process and lack of infrastructure-as-code practices suggest a potential skill gap in modern cloud operations and database management.

**Identified Skill Gaps and Training Needs**:

| Skill Area | Current State | Required Future State | Training Recommendation |
| :--- | :--- | :--- | :--- |
| **Database Migration** | Manual, file-based backup/restore process documented [^3]. | Proficiency with automated, tool-driven migration services. | Hands-on training with **Google Cloud Database Migration Service**. |
| **Cloud Storage** | No evidence of object storage usage; files are stored in the database [^4]. | Understanding of object storage concepts, SDKs, and security (signed URLs, IAM). | Focused workshop on **Google Cloud Storage**, including SDK integration and lifecycle management. |
| **Security** | Secrets managed via environment variables or in-file defaults [^8]. | Expertise in centralized secret management and IAM policies. | Training on **Google Cloud Secret Manager** and IAM best practices. |
| **Infrastructure** | Manual setup implied; no infrastructure-as-code (e.g., Terraform) present. | Ability to define and manage cloud infrastructure programmatically. | Introduction to **Terraform** for provisioning Google Cloud resources like Cloud SQL and Cloud Storage. |

##### Managing Change

The transition to a managed cloud environment requires a cultural shift. The team must move from managing servers to managing services. This involves embracing DevOps principles, taking ownership of the operational performance and cost of their code in the cloud, and collaborating across development and operations to ensure a smooth and successful modernization journey.

[^1]: build.gradle: `spring-boot-starter-data-jpa` - The project uses Spring Data JPA, which abstracts database interactions and manages schema based on entity definitions.
[^2]: src/main/resources/1.sql: `UPDATE t_user_category SET type = 100 where type = 0;` - This SQL script contains manual DDL/DML statements for updating the schema and data, indicating that not all schema changes are handled by JPA.
[^3]: notes/data_backup.txt: `要把存储引擎改为 MyISAM` - This note details a manual backup process that involves changing the database storage engine to MyISAM for file-based copying, which is a highly risky and outdated practice.
[^4]: src/main/java/cn/biq/mn/flowfile/FlowFile.java: `@Column(columnDefinition = "LONGBLOB", nullable = false)` - The `FlowFile` entity is defined to store file content directly in the database using a `LONGBLOB` column.
[^5]: src/main/java/cn/biq/mn/balanceflow/BalanceFlowService.java: `addFile(Integer id, MultipartFile file)` - This service method handles file uploads and would need to be refactored to upload files to Google Cloud Storage instead of saving them to the database.
[^6]: .github/workflows/docker-image-ali.yml: `registry: registry.cn-hangzhou.aliyuncs.com` - This GitHub Actions workflow pushes Docker images to an Alibaba Cloud Container Registry, indicating a potential deployment presence in the China region.
[^7]: src/main/java/cn/biq/mn/security/SecurityConfig.java: `return new BCryptPasswordEncoder();` - The application configures BCrypt for password encoding, which is a strong and recommended hashing algorithm.
[^8]: src/main/resources/application.properties: `spring.datasource.password=${DB_PASSWORD:78p7gkc1}` - The database password configuration uses an environment variable but has a hardcoded default value, which is a security risk.

### Modernization Strategies
This section outlines a two-phase modernization strategy for the application's data layer. The plan begins with low-effort, high-impact improvements and progresses to a comprehensive, long-term roadmap for migrating to and optimizing on Google Cloud.

#### Short-Term Improvements (Low-Hanging Fruit)

The following actions address immediate challenges and establish a solid foundation for future modernization efforts. These steps are designed for minimal disruption and can be implemented quickly to yield immediate benefits in performance, security, and operational efficiency.

| Action | Expected Outcome | Complexity |
| :--- | :--- | :--- |
| **Migrate MySQL Database to Cloud SQL** | Reduces database management overhead by moving from a self-managed MySQL instance [^1] to a fully managed service. Simplifies backups, patching, and scaling. | Moderate |
| **Externalize Secrets and Credentials** | Enhances security by removing hardcoded database credentials from the `application.properties` file [^1]. Prepares the application for secure configuration management using a service like Google Cloud Secret Manager. | Low |
| **Offload File Storage to Google Cloud Storage** | Improves scalability and performance by moving file uploads from the database (stored as `byte[]` arrays [^2]) to a dedicated object storage service. This refactoring of `FlowFileService` [^3] will reduce database load and size. | Moderate |
| **Standardize Docker Build Process** | Simplifies the CI/CD pipeline by consolidating multiple Docker build workflows [^4] into a single, optimized multi-stage `Dockerfile`. This ensures consistent, lean, and secure container images. | Low |
| **Implement a Database Migration Tool** | Replaces the brittle startup SQL script runner [^5] with a robust migration tool like Flyway or Liquibase. This provides version-controlled, automated, and reliable schema management. | Moderate |

#### Long-Term Modernization Plan

This long-term roadmap builds upon the short-term improvements to create a scalable, resilient, and cost-efficient data architecture on Google Cloud. The initiatives focus on fully leveraging managed services, implementing robust operational practices, and ensuring data security and compliance.

| Initiative | Strategic Impact | Resource Intensity | Dependencies |
| :--- | :--- | :--- | :--- |
| **Fully Leverage Managed Database Capabilities** | **Reliability & Scalability**: Enhances data resilience and performance by configuring high availability (HA), automated backups, point-in-time recovery (PITR), and read replicas in Cloud SQL. | Moderate | Successful migration of the database to Cloud SQL. |
| **Refactor Data Access for Performance** | **Performance**: Optimizes query performance under load by refactoring the data access layer. This could involve implementing read/write splitting to direct read-heavy queries (e.g., from `ReportService`) to Cloud SQL read replicas. | Moderate | Implementation of Cloud SQL read replicas. Understanding of the application's query patterns. |
| **Establish Centralized Logging and Monitoring** | **Operational Excellence**: Provides deep visibility into application and database performance by integrating with Google Cloud's operations suite (Cloud Logging, Cloud Monitoring). Enables proactive issue detection and performance tuning. | Minimal | Application deployed on Google Cloud infrastructure. |
| **Implement a Comprehensive Disaster Recovery (DR) Strategy** | **Business Continuity**: Protects against regional outages and data loss by using Cloud SQL's cross-region replication and configuring Google Cloud Storage for cross-region data replication. | Moderate | Key infrastructure (Cloud SQL, Cloud Storage) established on Google Cloud. |
| **Enhance Data Security and GDPR Compliance** | **Security & Compliance**: Strengthens data protection by enforcing encryption at rest and in transit, using Identity and Access Management (IAM) for least-privilege access, and auditing data access to meet GDPR requirements for financial data. | Significant | A full data audit and understanding of data lifecycle within the application. |
| **Automate Infrastructure with Infrastructure as Code (IaC)** | **Efficiency & Consistency**: Ensures consistent, repeatable, and automated deployment of all data layer resources (Cloud SQL, Cloud Storage, Secret Manager) by adopting Terraform or Google Cloud Deployment Manager. | Moderate | Finalized cloud architecture design. |

[^1]: src/main/resources/application.properties: `spring.datasource.password=${DB_PASSWORD:******}` - Contains the JDBC URL and credentials for the MySQL database.
[^2]: src/main/java/cn/biq/mn/flowfile/FlowFile.java: `private byte[] data;` - This entity field stores uploaded file contents directly in the database as a large object (BLOB).
[^3]: src/main/java/cn/biq/mn/flowfile/FlowFileService.java: `addFile` - This method contains the logic for persisting uploaded files and would be the target for refactoring to use Google Cloud Storage.
[^4]: .github/workflows/: `docker-image-ali.yml`, `docker-image-all.yml`, etc. - Multiple workflow files indicate a fragmented and potentially inconsistent process for building and pushing Docker images.
[^5]: src/main/java/cn/biq/mn/SqlScriptRunner.java: `run` method - Executes a raw SQL script on application startup to apply database schema changes, a practice that is not robust or version-controlled.

